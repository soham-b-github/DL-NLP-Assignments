{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d36b05c3-aed6-4026-8285-357da386d222",
   "metadata": {},
   "source": [
    "## Installing dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "be35a98e-bcc2-4af6-9b37-0ddab336adc4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: torchsummary in /home/soham/.local/lib/python3.12/site-packages (1.5.1)\n",
      "Requirement already satisfied: typing in /home/soham/.local/lib/python3.12/site-packages (3.7.4.3)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install torchsummary typing --break-system-packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "62f8a9ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "-----------------------------------------------------------------------------\n",
    "Transformer using pytorch and numpy\n",
    "-----------------------------------------------------------------------------\n",
    "AUTHOR: Soumitra Samanta (soumitra.samanta@gm.rkmvu.ac.in)\n",
    "-----------------------------------------------------------------------------\n",
    "Package required:\n",
    "Numpy: https://numpy.org/\n",
    "Matplotlib: https://matplotlib.org\n",
    "-----------------------------------------------------------------------------\n",
    "\"\"\"\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch import Tensor\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torchsummary import summary\n",
    "import math\n",
    "\n",
    "from typing import Tuple\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8dc946e2-9bfe-4d39-bd89-13486e5776be",
   "metadata": {},
   "outputs": [],
   "source": [
    "class self_attention_layer(nn.Module):\n",
    "    \"\"\"\n",
    "    Self attention layer\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        dims_embd: int,\n",
    "    )->None:\n",
    "        \"\"\"\n",
    "        Self attention class initialization\n",
    "        \n",
    "        Inpout:\n",
    "            - dims_embd (int): Embedding dimension\n",
    "        \"\"\"\n",
    "        \n",
    "        super().__init__()\n",
    "        self.dims_embd_ = dims_embd\n",
    "        ############################################################################\n",
    "        #                             Your code will be here                       #\n",
    "        #--------------------------------------------------------------------------#\n",
    "        self.W_q_ = nn.Linear(dims_embd, dims_embd)\n",
    "        self.W_k_ = nn.Linear(dims_embd, dims_embd)\n",
    "        self.W_v_ = nn.Linear(dims_embd, dims_embd)\n",
    "        \n",
    "        \n",
    "        #--------------------------------------------------------------------------#\n",
    "        #                             End of your code                             #\n",
    "        ############################################################################\n",
    "        \n",
    "    def forward(\n",
    "        self, \n",
    "        x: Tensor \n",
    "    )->Tensor:\n",
    "        \"\"\"\n",
    "        Forward pass for the self attention layer\n",
    "        \n",
    "        Imput:\n",
    "            - x (torch tensor): Input data\n",
    "            \n",
    "        Output:\n",
    "        \n",
    "        \"\"\"\n",
    "        \n",
    "        y = []\n",
    "        ############################################################################\n",
    "        #                             Your code will be here                       #\n",
    "        #--------------------------------------------------------------------------#\n",
    "\n",
    "        Q = self.W_q_(x)\n",
    "        K = self.W_k_(x)\n",
    "        V = self.W_v_(x)\n",
    "        \n",
    "        d = self.dims_embd_\n",
    "        attention_scores = torch.matmul(Q,K.transpose(-2,-1))/math.sqrt(d)\n",
    "\n",
    "        attention_weights = F.softmax(attention_scores, dim=-1)\n",
    "        y = torch.matmul(attention_weights, V)  \n",
    "        \n",
    "        #--------------------------------------------------------------------------#\n",
    "        #                             End of your code                             #\n",
    "        ############################################################################\n",
    "    \n",
    "        return y\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "64e245cd-7b04-4385-8e32-90c20c3a384a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class transformer_block_encoder(nn.Module):\n",
    "    \"\"\"\n",
    "    Transformer single block\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        dims_embd: int,\n",
    "        num_hidden_nodes_ffnn: int = 2048,\n",
    "        dropout_prob: float = 0.0\n",
    "    )->None:\n",
    "        \"\"\"\n",
    "        Transformer single block class initialization\n",
    "        \n",
    "        Inpout:\n",
    "            - dims_embd (int):             Embedding dimension\n",
    "            - num_hidden_nodes_ffnn (int): Number of neurons in the fed-forward layer\n",
    "            - dropout_prob (float):        Dropout probability in liner layers\n",
    "        \"\"\"\n",
    "        \n",
    "        super().__init__()\n",
    "        \n",
    "        ############################################################################\n",
    "        #                             Your code will be here                       #\n",
    "        #--------------------------------------------------------------------------#\n",
    "        self.attention_ = self_attention_layer(dims_embd)\n",
    "        \n",
    "        self.layer_norm1_ = nn.LayerNorm(dims_embd)\n",
    "        self.layer_norm2_ = nn.LayerNorm(dims_embd)\n",
    "        \n",
    "        self.ffnn_ = nn.Sequential(\n",
    "            nn.Linear(dims_embd, num_hidden_nodes_ffnn),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(num_hidden_nodes_ffnn, dims_embd)\n",
    "        )\n",
    "        self.droput_ops_ = nn.Dropout(dropout_prob)\n",
    "        \n",
    "        self.dims_embd_ = dims_embd\n",
    "        self.num_hidden_nodes_ffnn_ = num_hidden_nodes_ffnn\n",
    "        self.dropout_prob_ = dropout_prob\n",
    "        \n",
    "        \n",
    "        #--------------------------------------------------------------------------#\n",
    "        #                             End of your code                             #\n",
    "        ############################################################################\n",
    "        \n",
    "    def forward(\n",
    "        self,\n",
    "        x: Tensor,\n",
    "    )->Tensor:\n",
    "        \"\"\"\n",
    "        Forward pass for the transformer block\n",
    "        \n",
    "        Imput:\n",
    "            - x (torch tensor): Input data\n",
    "            \n",
    "        Output:\n",
    "        \n",
    "        \"\"\"\n",
    "        \n",
    "        ############################################################################\n",
    "        #                             Your code will be here                       #\n",
    "        #--------------------------------------------------------------------------#\n",
    "\n",
    "        # Inside Encoder Block of transformer: We execute the following\n",
    "        # Self_attention_layer -> Add residuals of self_attention -> Layer Normalize\n",
    "        # -> Feed Forward -> Add residuals of feed_forward -> Layer Normalize\n",
    "        \n",
    "        self_attention = self.attention_(x)\n",
    "        x = x + self.droput_ops_(self_attention)\n",
    "        x = self.layer_norm1_(x)\n",
    "    \n",
    "        feed_forward = self.ffnn_(x)\n",
    "        x = x + self.droput_ops_(feed_forward)\n",
    "        x = self.layer_norm2_(x)\n",
    "    \n",
    "        #--------------------------------------------------------------------------#\n",
    "        #                             End of your code                             #\n",
    "        ############################################################################\n",
    "        \n",
    "        return x\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8d0b1cd8-42be-4b18-801b-bfdd78510275",
   "metadata": {},
   "outputs": [],
   "source": [
    "class transformer_encoder(nn.Module):\n",
    "    \"\"\"\n",
    "    Transformer encoder module\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        dims_embd: int,\n",
    "        num_hidden_nodes_ffnn: int = 2048,\n",
    "        dropout_prob: float = 0.0,\n",
    "        num_layers_encoder: int = 2\n",
    "    )->None:\n",
    "        \"\"\"\n",
    "        Transformer encoder class initialization\n",
    "        \n",
    "        Inpout:\n",
    "            - dims_embd (int):             Embedding dimension\n",
    "            - num_hidden_nodes_ffnn (int): Number of neurons in the fed-forward layer\n",
    "            - dropout_prob (float):        Dropout probability in liner layers\n",
    "            - num_layers_encoder (int):    Number encoder blocks\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        \n",
    "        ############################################################################\n",
    "        #                             Your code will be here                       #\n",
    "        #--------------------------------------------------------------------------#\n",
    "        self.trs_endr_blocks_ = nn.ModuleList(\n",
    "            [\n",
    "                transformer_block_encoder(dims_embd, num_hidden_nodes_ffnn, dropout_prob) for _ in range(num_layers_encoder)\n",
    "            ]\n",
    "        )\n",
    "        \n",
    "        self.num_layers_encoder_ = num_layers_encoder\n",
    "        \n",
    "        \n",
    "        #--------------------------------------------------------------------------#\n",
    "        #                             End of your code                             #\n",
    "        ############################################################################\n",
    "    \n",
    "    def forward(\n",
    "        self,\n",
    "        x: Tensor,\n",
    "    )->Tensor:\n",
    "        \"\"\"\n",
    "        Forward pass for the transformer encoder\n",
    "        \n",
    "        Imput:\n",
    "            - x (torch tensor): Input data\n",
    "            \n",
    "        Output:\n",
    "        \n",
    "        \"\"\"\n",
    "        \n",
    "        ############################################################################\n",
    "        #                             Your code will be here                       #\n",
    "        #--------------------------------------------------------------------------#\n",
    "        \n",
    "        for block in self.trs_endr_blocks_:\n",
    "            x = block(x)\n",
    "        \n",
    "        #--------------------------------------------------------------------------#\n",
    "        #                             End of your code                             #\n",
    "        ############################################################################\n",
    "        \n",
    "        return x\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6be4eacf-1911-4c09-9f74-7f0a5bd94663",
   "metadata": {},
   "outputs": [],
   "source": [
    "class cross_attention_layer(nn.Module):\n",
    "    \"\"\"\n",
    "    Cross attention layer\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        dims_embd: int,\n",
    "    )->None:\n",
    "        \"\"\"\n",
    "        Cross attention class initialization\n",
    "        \n",
    "        Inpout:\n",
    "            - dims_embd (int): Embedding dimension\n",
    "        \"\"\"\n",
    "        \n",
    "        super().__init__()\n",
    "        \n",
    "        ############################################################################\n",
    "        #                             Your code will be here                       #\n",
    "        #--------------------------------------------------------------------------#\n",
    "        \n",
    "        self.W_q_ = nn.Linear(dims_embd, dims_embd)\n",
    "        self.W_k_ = nn.Linear(dims_embd, dims_embd)\n",
    "        self.W_v_ = nn.Linear(dims_embd, dims_embd)\n",
    "        \n",
    "        self.dims_embd_ = dims_embd\n",
    "        \n",
    "        \n",
    "        #--------------------------------------------------------------------------#\n",
    "        #                             End of your code                             #\n",
    "        ############################################################################\n",
    "        \n",
    "    def forward(\n",
    "        self, \n",
    "        x: Tensor,\n",
    "        y: Tensor\n",
    "    )->Tensor:\n",
    "        \"\"\"\n",
    "        Forward pass for the cross-attention layer\n",
    "        \n",
    "        Imput:\n",
    "            - x (torch tensor): Input encoder data\n",
    "            - y (torch tensor): Input decoder data\n",
    "            \n",
    "        Output:\n",
    "        \n",
    "        \"\"\"\n",
    "        \n",
    "        ############################################################################\n",
    "        #                             Your code will be here                       #\n",
    "        #--------------------------------------------------------------------------#\n",
    "        \n",
    "        # Q calculated using decoder input data\n",
    "        Q = self.W_q_(y)\n",
    "\n",
    "        # K and V calculated using encoder input data\n",
    "        K = self.W_k_(x)\n",
    "        V = self.W_v_(x)\n",
    "        \n",
    "        scores = torch.matmul(Q, K.transpose(-2,-1)) / (self.dims_embd_**0.5)\n",
    "        attn_weights = torch.softmax(scores, dim=-1)\n",
    "    \n",
    "        y = torch.matmul(attn_weights, V)\n",
    "        \n",
    "        #--------------------------------------------------------------------------#\n",
    "        #                             End of your code                             #\n",
    "        ############################################################################\n",
    "    \n",
    "        return y\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f80074a5-d052-45c9-9cd7-b322e863887e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class transformer_block_decoder(nn.Module):\n",
    "    \"\"\"\n",
    "    Transformer single decoder block\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        dims_embd: int,\n",
    "        num_hidden_nodes_ffnn: int = 2048,\n",
    "        dropout_prob: float = 0.0\n",
    "    )->None:\n",
    "        \"\"\"\n",
    "        Transformer single block class initialization\n",
    "        \n",
    "        Inpout:\n",
    "            - dims_embd (int):             Embedding dimension\n",
    "            - num_hidden_nodes_ffnn (int): Number of neurons in the fed-forward layer\n",
    "            - dropout_prob (float):        Dropout probability in liner layers\n",
    "        \"\"\"\n",
    "        \n",
    "        super().__init__()\n",
    "        \n",
    "        ############################################################################\n",
    "        #                             Your code will be here                       #\n",
    "        #--------------------------------------------------------------------------#\n",
    "        \n",
    "        self.attention_ = self_attention_layer(dims_embd)\n",
    "        self.cross_attention_ = cross_attention_layer(dims_embd)\n",
    "        \n",
    "        self.layer_norm1_ = nn.LayerNorm(dims_embd)\n",
    "        self.layer_norm2_ = nn.LayerNorm(dims_embd)\n",
    "        self.layer_norm3_ = nn.LayerNorm(dims_embd)\n",
    "        \n",
    "        self.ffnn_ = nn.Sequential(\n",
    "            nn.Linear(dims_embd, num_hidden_nodes_ffnn),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(num_hidden_nodes_ffnn, dims_embd)\n",
    "        )\n",
    "        self.droput_ops_ = nn.Dropout(dropout_prob)\n",
    "        \n",
    "        self.dims_embd_ = dims_embd\n",
    "        self.num_hidden_nodes_ffnn_ = num_hidden_nodes_ffnn\n",
    "        self.dropout_prob_ = dropout_prob\n",
    "        \n",
    "        #--------------------------------------------------------------------------#\n",
    "        #                             End of your code                             #\n",
    "        ############################################################################\n",
    "        \n",
    "    def forward(\n",
    "        self,\n",
    "        x: Tensor,\n",
    "        y: Tensor\n",
    "    )->Tensor:\n",
    "        \"\"\"\n",
    "        Forward pass for the transformer block\n",
    "        \n",
    "        Imput:\n",
    "            - x (torch tensor): Input encoder data\n",
    "            - y (torch tensor): Input decoder data\n",
    "            \n",
    "        Output:\n",
    "        \n",
    "        \"\"\"\n",
    "        \n",
    "        ############################################################################\n",
    "        #                             Your code will be here                       #\n",
    "        #--------------------------------------------------------------------------#\n",
    "\n",
    "        # Self_attention first\n",
    "        y1 = self.attention_(y)\n",
    "        y = self.layer_norm1_(y + self.droput_ops_(y1))\n",
    "    \n",
    "        # Then cross_attention is decoder attending to encoder\n",
    "        y1 = self.cross_attention_(x, y)\n",
    "        y = self.layer_norm2_(y + self.droput_ops_(y1))\n",
    "    \n",
    "        # and finally feed_forward_network\n",
    "        y1 = self.ffnn_(y)\n",
    "        y = self.layer_norm3_(y + self.droput_ops_(y1))\n",
    "    \n",
    "        #--------------------------------------------------------------------------#\n",
    "        #                             End of your code                             #\n",
    "        ############################################################################\n",
    "        \n",
    "        return y\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "81fc384d-caa5-4dd8-955f-ba133c9372a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class transformer_decoder(nn.Module):\n",
    "    \"\"\"\n",
    "    Transformer decoder module\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        dims_embd: int,\n",
    "        num_hidden_nodes_ffnn: int = 2048,\n",
    "        dropout_prob: float = 0.0,\n",
    "        num_layers_decoder: int = 2\n",
    "    )->None:\n",
    "        \"\"\"\n",
    "        Transformer decoder class initialization\n",
    "        \n",
    "        Inpout:\n",
    "            - dims_embd (int):             Embedding dimension\n",
    "            - num_hidden_nodes_ffnn (int): Number of neurons in the fed-forward layer\n",
    "            - dropout_prob (float):        Dropout probability in liner layers\n",
    "            - num_layers_decoder (int):    Number decoder blocks\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        \n",
    "        ############################################################################\n",
    "        #                             Your code will be here                       #\n",
    "        #--------------------------------------------------------------------------#\n",
    "        self.trs_dcdr_blocks_ = nn.ModuleList(\n",
    "            [\n",
    "                transformer_block_decoder(dims_embd, num_hidden_nodes_ffnn, dropout_prob) for _ in range(num_layers_decoder)\n",
    "            ]\n",
    "        )\n",
    "        \n",
    "        self.num_layers_decoder_ = num_layers_decoder\n",
    "        #--------------------------------------------------------------------------#\n",
    "        #                             End of your code                             #\n",
    "        ############################################################################\n",
    "        \n",
    "    def forward(\n",
    "        self,\n",
    "        x: Tensor,\n",
    "        y: Tensor\n",
    "    )->Tensor:\n",
    "        \"\"\"\n",
    "        Forward pass for the transformer encoder\n",
    "        \n",
    "        Imput:\n",
    "            - x (torch tensor): Input encoder data\n",
    "            - y (torch tensor): Input decoder data\n",
    "            \n",
    "        Output:\n",
    "        \n",
    "        \"\"\"\n",
    "        \n",
    "        ############################################################################\n",
    "        #                             Your code will be here                       #\n",
    "        #--------------------------------------------------------------------------#\n",
    "        \n",
    "        for block in self.trs_dcdr_blocks_:\n",
    "            x = block(x,y)\n",
    "        \n",
    "        #--------------------------------------------------------------------------#\n",
    "        #                             End of your code                             #\n",
    "        ############################################################################\n",
    "        \n",
    "        return x\n",
    "        \n",
    "       \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cc004c69-b48e-4084-b77b-0ced2bf57b83",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "Self-attention layer models is: \n",
      "self_attention_layer(\n",
      "  (W_q_): Linear(in_features=10, out_features=10, bias=True)\n",
      "  (W_k_): Linear(in_features=10, out_features=10, bias=True)\n",
      "  (W_v_): Linear(in_features=10, out_features=10, bias=True)\n",
      ")\n",
      "----------------------------------------------------------------------\n",
      "Self-attention layer input size: torch.Size([5, 100, 10])\n",
      "Self-attention layer output size: torch.Size([5, 100, 10])\n",
      "----------------------------------------------------------------------\n",
      "======================================================================\n",
      "Transformer block models is: \n",
      "transformer_block_encoder(\n",
      "  (attention_): self_attention_layer(\n",
      "    (W_q_): Linear(in_features=10, out_features=10, bias=True)\n",
      "    (W_k_): Linear(in_features=10, out_features=10, bias=True)\n",
      "    (W_v_): Linear(in_features=10, out_features=10, bias=True)\n",
      "  )\n",
      "  (layer_norm1_): LayerNorm((10,), eps=1e-05, elementwise_affine=True)\n",
      "  (layer_norm2_): LayerNorm((10,), eps=1e-05, elementwise_affine=True)\n",
      "  (ffnn_): Sequential(\n",
      "    (0): Linear(in_features=10, out_features=1024, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=1024, out_features=10, bias=True)\n",
      "  )\n",
      "  (droput_ops_): Dropout(p=0.2, inplace=False)\n",
      ")\n",
      "----------------------------------------------------------------------\n",
      "Transformer block models summary:\n",
      "----------------------------------------------------------------------\n",
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Linear-1              [-1, 100, 10]             110\n",
      "            Linear-2              [-1, 100, 10]             110\n",
      "            Linear-3              [-1, 100, 10]             110\n",
      "self_attention_layer-4              [-1, 100, 10]               0\n",
      "           Dropout-5              [-1, 100, 10]               0\n",
      "         LayerNorm-6              [-1, 100, 10]              20\n",
      "            Linear-7            [-1, 100, 1024]          11,264\n",
      "              ReLU-8            [-1, 100, 1024]               0\n",
      "            Linear-9              [-1, 100, 10]          10,250\n",
      "          Dropout-10              [-1, 100, 10]               0\n",
      "        LayerNorm-11              [-1, 100, 10]              20\n",
      "================================================================\n",
      "Total params: 21,884\n",
      "Trainable params: 21,884\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.00\n",
      "Forward/backward pass size (MB): 1.63\n",
      "Params size (MB): 0.08\n",
      "Estimated Total Size (MB): 1.72\n",
      "----------------------------------------------------------------\n",
      "----------------------------------------------------------------------\n",
      "Transformer block input size: torch.Size([5, 100, 10])\n",
      "Transformer block output size: torch.Size([5, 100, 10])\n",
      "----------------------------------------------------------------------\n",
      "======================================================================\n",
      "Transformer encoder models is: \n",
      "transformer_encoder(\n",
      "  (trs_endr_blocks_): ModuleList(\n",
      "    (0-1): 2 x transformer_block_encoder(\n",
      "      (attention_): self_attention_layer(\n",
      "        (W_q_): Linear(in_features=10, out_features=10, bias=True)\n",
      "        (W_k_): Linear(in_features=10, out_features=10, bias=True)\n",
      "        (W_v_): Linear(in_features=10, out_features=10, bias=True)\n",
      "      )\n",
      "      (layer_norm1_): LayerNorm((10,), eps=1e-05, elementwise_affine=True)\n",
      "      (layer_norm2_): LayerNorm((10,), eps=1e-05, elementwise_affine=True)\n",
      "      (ffnn_): Sequential(\n",
      "        (0): Linear(in_features=10, out_features=1024, bias=True)\n",
      "        (1): ReLU()\n",
      "        (2): Linear(in_features=1024, out_features=10, bias=True)\n",
      "      )\n",
      "      (droput_ops_): Dropout(p=0.2, inplace=False)\n",
      "    )\n",
      "  )\n",
      ")\n",
      "----------------------------------------------------------------------\n",
      "Transformer encoder models summary:\n",
      "----------------------------------------------------------------------\n",
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Linear-1              [-1, 100, 10]             110\n",
      "            Linear-2              [-1, 100, 10]             110\n",
      "            Linear-3              [-1, 100, 10]             110\n",
      "self_attention_layer-4              [-1, 100, 10]               0\n",
      "           Dropout-5              [-1, 100, 10]               0\n",
      "         LayerNorm-6              [-1, 100, 10]              20\n",
      "            Linear-7            [-1, 100, 1024]          11,264\n",
      "              ReLU-8            [-1, 100, 1024]               0\n",
      "            Linear-9              [-1, 100, 10]          10,250\n",
      "          Dropout-10              [-1, 100, 10]               0\n",
      "        LayerNorm-11              [-1, 100, 10]              20\n",
      "transformer_block_encoder-12              [-1, 100, 10]               0\n",
      "           Linear-13              [-1, 100, 10]             110\n",
      "           Linear-14              [-1, 100, 10]             110\n",
      "           Linear-15              [-1, 100, 10]             110\n",
      "self_attention_layer-16              [-1, 100, 10]               0\n",
      "          Dropout-17              [-1, 100, 10]               0\n",
      "        LayerNorm-18              [-1, 100, 10]              20\n",
      "           Linear-19            [-1, 100, 1024]          11,264\n",
      "             ReLU-20            [-1, 100, 1024]               0\n",
      "           Linear-21              [-1, 100, 10]          10,250\n",
      "          Dropout-22              [-1, 100, 10]               0\n",
      "        LayerNorm-23              [-1, 100, 10]              20\n",
      "transformer_block_encoder-24              [-1, 100, 10]               0\n",
      "================================================================\n",
      "Total params: 43,768\n",
      "Trainable params: 43,768\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.00\n",
      "Forward/backward pass size (MB): 3.28\n",
      "Params size (MB): 0.17\n",
      "Estimated Total Size (MB): 3.45\n",
      "----------------------------------------------------------------\n",
      "----------------------------------------------------------------------\n",
      "Transformer encoder input size: torch.Size([5, 100, 10])\n",
      "Transformer encoder output size: torch.Size([5, 100, 10])\n",
      "----------------------------------------------------------------------\n",
      "======================================================================\n",
      "Cross-attention layer models is: \n",
      "cross_attention_layer(\n",
      "  (W_q_): Linear(in_features=10, out_features=10, bias=True)\n",
      "  (W_k_): Linear(in_features=10, out_features=10, bias=True)\n",
      "  (W_v_): Linear(in_features=10, out_features=10, bias=True)\n",
      ")\n",
      "----------------------------------------------------------------------\n",
      "Cross-attention layer input size: torch.Size([5, 100, 10])\n",
      "Cross-attention layer output size: torch.Size([5, 100, 10])\n",
      "----------------------------------------------------------------------\n",
      "======================================================================\n",
      "Transformer decoder block models is: \n",
      "transformer_block_decoder(\n",
      "  (attention_): self_attention_layer(\n",
      "    (W_q_): Linear(in_features=10, out_features=10, bias=True)\n",
      "    (W_k_): Linear(in_features=10, out_features=10, bias=True)\n",
      "    (W_v_): Linear(in_features=10, out_features=10, bias=True)\n",
      "  )\n",
      "  (cross_attention_): cross_attention_layer(\n",
      "    (W_q_): Linear(in_features=10, out_features=10, bias=True)\n",
      "    (W_k_): Linear(in_features=10, out_features=10, bias=True)\n",
      "    (W_v_): Linear(in_features=10, out_features=10, bias=True)\n",
      "  )\n",
      "  (layer_norm1_): LayerNorm((10,), eps=1e-05, elementwise_affine=True)\n",
      "  (layer_norm2_): LayerNorm((10,), eps=1e-05, elementwise_affine=True)\n",
      "  (layer_norm3_): LayerNorm((10,), eps=1e-05, elementwise_affine=True)\n",
      "  (ffnn_): Sequential(\n",
      "    (0): Linear(in_features=10, out_features=1024, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=1024, out_features=10, bias=True)\n",
      "  )\n",
      "  (droput_ops_): Dropout(p=0.2, inplace=False)\n",
      ")\n",
      "----------------------------------------------------------------------\n",
      "Transformer decoder block models summary:\n",
      "----------------------------------------------------------------------\n",
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Linear-1              [-1, 100, 10]             110\n",
      "            Linear-2              [-1, 100, 10]             110\n",
      "            Linear-3              [-1, 100, 10]             110\n",
      "self_attention_layer-4              [-1, 100, 10]               0\n",
      "           Dropout-5              [-1, 100, 10]               0\n",
      "         LayerNorm-6              [-1, 100, 10]              20\n",
      "            Linear-7              [-1, 100, 10]             110\n",
      "            Linear-8              [-1, 100, 10]             110\n",
      "            Linear-9              [-1, 100, 10]             110\n",
      "cross_attention_layer-10              [-1, 100, 10]               0\n",
      "          Dropout-11              [-1, 100, 10]               0\n",
      "        LayerNorm-12              [-1, 100, 10]              20\n",
      "           Linear-13            [-1, 100, 1024]          11,264\n",
      "             ReLU-14            [-1, 100, 1024]               0\n",
      "           Linear-15              [-1, 100, 10]          10,250\n",
      "          Dropout-16              [-1, 100, 10]               0\n",
      "        LayerNorm-17              [-1, 100, 10]              20\n",
      "================================================================\n",
      "Total params: 22,234\n",
      "Trainable params: 22,234\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 3.81\n",
      "Forward/backward pass size (MB): 1.68\n",
      "Params size (MB): 0.08\n",
      "Estimated Total Size (MB): 5.58\n",
      "----------------------------------------------------------------\n",
      "----------------------------------------------------------------------\n",
      "Transformer block input size: torch.Size([5, 100, 10])\n",
      "Transformer block output size: torch.Size([5, 100, 10])\n",
      "----------------------------------------------------------------------\n",
      "======================================================================\n",
      "Transformer decoder models is: \n",
      "transformer_decoder(\n",
      "  (trs_dcdr_blocks_): ModuleList(\n",
      "    (0-1): 2 x transformer_block_decoder(\n",
      "      (attention_): self_attention_layer(\n",
      "        (W_q_): Linear(in_features=10, out_features=10, bias=True)\n",
      "        (W_k_): Linear(in_features=10, out_features=10, bias=True)\n",
      "        (W_v_): Linear(in_features=10, out_features=10, bias=True)\n",
      "      )\n",
      "      (cross_attention_): cross_attention_layer(\n",
      "        (W_q_): Linear(in_features=10, out_features=10, bias=True)\n",
      "        (W_k_): Linear(in_features=10, out_features=10, bias=True)\n",
      "        (W_v_): Linear(in_features=10, out_features=10, bias=True)\n",
      "      )\n",
      "      (layer_norm1_): LayerNorm((10,), eps=1e-05, elementwise_affine=True)\n",
      "      (layer_norm2_): LayerNorm((10,), eps=1e-05, elementwise_affine=True)\n",
      "      (layer_norm3_): LayerNorm((10,), eps=1e-05, elementwise_affine=True)\n",
      "      (ffnn_): Sequential(\n",
      "        (0): Linear(in_features=10, out_features=1024, bias=True)\n",
      "        (1): ReLU()\n",
      "        (2): Linear(in_features=1024, out_features=10, bias=True)\n",
      "      )\n",
      "      (droput_ops_): Dropout(p=0.2, inplace=False)\n",
      "    )\n",
      "  )\n",
      ")\n",
      "----------------------------------------------------------------------\n",
      "Transformer decoder models summary:\n",
      "----------------------------------------------------------------------\n",
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Linear-1              [-1, 100, 10]             110\n",
      "            Linear-2              [-1, 100, 10]             110\n",
      "            Linear-3              [-1, 100, 10]             110\n",
      "self_attention_layer-4              [-1, 100, 10]               0\n",
      "           Dropout-5              [-1, 100, 10]               0\n",
      "         LayerNorm-6              [-1, 100, 10]              20\n",
      "            Linear-7              [-1, 100, 10]             110\n",
      "            Linear-8              [-1, 100, 10]             110\n",
      "            Linear-9              [-1, 100, 10]             110\n",
      "cross_attention_layer-10              [-1, 100, 10]               0\n",
      "          Dropout-11              [-1, 100, 10]               0\n",
      "        LayerNorm-12              [-1, 100, 10]              20\n",
      "           Linear-13            [-1, 100, 1024]          11,264\n",
      "             ReLU-14            [-1, 100, 1024]               0\n",
      "           Linear-15              [-1, 100, 10]          10,250\n",
      "          Dropout-16              [-1, 100, 10]               0\n",
      "        LayerNorm-17              [-1, 100, 10]              20\n",
      "transformer_block_decoder-18              [-1, 100, 10]               0\n",
      "           Linear-19              [-1, 100, 10]             110\n",
      "           Linear-20              [-1, 100, 10]             110\n",
      "           Linear-21              [-1, 100, 10]             110\n",
      "self_attention_layer-22              [-1, 100, 10]               0\n",
      "          Dropout-23              [-1, 100, 10]               0\n",
      "        LayerNorm-24              [-1, 100, 10]              20\n",
      "           Linear-25              [-1, 100, 10]             110\n",
      "           Linear-26              [-1, 100, 10]             110\n",
      "           Linear-27              [-1, 100, 10]             110\n",
      "cross_attention_layer-28              [-1, 100, 10]               0\n",
      "          Dropout-29              [-1, 100, 10]               0\n",
      "        LayerNorm-30              [-1, 100, 10]              20\n",
      "           Linear-31            [-1, 100, 1024]          11,264\n",
      "             ReLU-32            [-1, 100, 1024]               0\n",
      "           Linear-33              [-1, 100, 10]          10,250\n",
      "          Dropout-34              [-1, 100, 10]               0\n",
      "        LayerNorm-35              [-1, 100, 10]              20\n",
      "transformer_block_decoder-36              [-1, 100, 10]               0\n",
      "================================================================\n",
      "Total params: 44,468\n",
      "Trainable params: 44,468\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 3.81\n",
      "Forward/backward pass size (MB): 3.37\n",
      "Params size (MB): 0.17\n",
      "Estimated Total Size (MB): 7.35\n",
      "----------------------------------------------------------------\n",
      "----------------------------------------------------------------------\n",
      "Transformer decoder input size: torch.Size([5, 100, 10])\n",
      "Transformer decoder output size: torch.Size([5, 100, 10])\n",
      "----------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "dims_embd = 10\n",
    "num_data_points = 100\n",
    "batch_size = 5\n",
    "num_hidden_nodes_ffnn = 1024\n",
    "dropout_prob = 0.2\n",
    "num_layers_encoder = 2\n",
    "\n",
    "x = torch.rand(batch_size, num_data_points, dims_embd)\n",
    "y = torch.rand(batch_size, num_data_points, dims_embd)\n",
    "\n",
    "# Test Self-attention layer and its input output size  \n",
    "print('='*70)\n",
    "model_self_attention_layer = self_attention_layer(dims_embd)\n",
    "print('Self-attention layer models is: \\n{}' .format(model_self_attention_layer))\n",
    "print('-'*70)\n",
    "\n",
    "y_bar = model_self_attention_layer(x)\n",
    "print('Self-attention layer input size: {}' .format(x.shape))\n",
    "print('Self-attention layer output size: {}' .format(y_bar.shape))\n",
    "print('-'*70)\n",
    "        \n",
    "# Test Transformer encoder block input output size \n",
    "print('='*70)\n",
    "model_transformer_block_encoder = transformer_block_encoder(dims_embd, num_hidden_nodes_ffnn, dropout_prob)\n",
    "print('Transformer block models is: \\n{}' .format(model_transformer_block_encoder))\n",
    "print('-'*70)\n",
    "print('Transformer block models summary:')\n",
    "print('-'*70)\n",
    "summary(model_transformer_block_encoder, (num_data_points, dims_embd, ), device=str(\"cpu\"))\n",
    "print('-'*70)\n",
    "\n",
    "y_bar = model_transformer_block_encoder(x)\n",
    "print('Transformer block input size: {}' .format(x.shape))\n",
    "print('Transformer block output size: {}' .format(y_bar.shape))  \n",
    "print('-'*70)\n",
    "\n",
    "# Test Transformer encoder input output size \n",
    "print('='*70)\n",
    "model_transformer_encoder = transformer_encoder(dims_embd, num_hidden_nodes_ffnn, dropout_prob, num_layers_encoder)\n",
    "print('Transformer encoder models is: \\n{}' .format(model_transformer_encoder))\n",
    "print('-'*70)\n",
    "print('Transformer encoder models summary:')\n",
    "print('-'*70)\n",
    "summary(model_transformer_encoder, (num_data_points, dims_embd, ), device=str(\"cpu\"))\n",
    "print('-'*70)\n",
    "\n",
    "y_bar = model_transformer_encoder(x)\n",
    "print('Transformer encoder input size: {}' .format(x.shape))\n",
    "print('Transformer encoder output size: {}' .format(y_bar.shape))  \n",
    "print('-'*70)\n",
    "\n",
    "# Test Cross-attention layer and its input output size  \n",
    "print('='*70)\n",
    "model_cross_attention_layer = cross_attention_layer(dims_embd)\n",
    "print('Cross-attention layer models is: \\n{}' .format(model_cross_attention_layer))\n",
    "print('-'*70)\n",
    "\n",
    "y_bar = model_cross_attention_layer(x, y)\n",
    "print('Cross-attention layer input size: {}' .format(x.shape))\n",
    "print('Cross-attention layer output size: {}' .format(y_bar.shape))\n",
    "print('-'*70)\n",
    "\n",
    "# Test Transformer decoder block input output size \n",
    "print('='*70)\n",
    "model_transformer_block_decoder = transformer_block_decoder(dims_embd, num_hidden_nodes_ffnn, dropout_prob)\n",
    "print('Transformer decoder block models is: \\n{}' .format(model_transformer_block_decoder))\n",
    "print('-'*70)\n",
    "print('Transformer decoder block models summary:')\n",
    "print('-'*70)\n",
    "summary(model_transformer_block_decoder, [(num_data_points, dims_embd, ), (num_data_points, dims_embd, )], device=str(\"cpu\"))\n",
    "print('-'*70)\n",
    "\n",
    "y_bar = model_transformer_block_decoder(x, y)\n",
    "print('Transformer block input size: {}' .format(x.shape))\n",
    "print('Transformer block output size: {}' .format(y_bar.shape))  \n",
    "print('-'*70)\n",
    "\n",
    "# Test Transformer decoder input output size \n",
    "print('='*70)\n",
    "model_transformer_decoder = transformer_decoder(dims_embd, num_hidden_nodes_ffnn, dropout_prob, num_layers_encoder)\n",
    "print('Transformer decoder models is: \\n{}' .format(model_transformer_decoder))\n",
    "print('-'*70)\n",
    "print('Transformer decoder models summary:')\n",
    "print('-'*70)\n",
    "summary(model_transformer_decoder, [(num_data_points, dims_embd, ), (num_data_points, dims_embd, )], device=str(\"cpu\"))\n",
    "print('-'*70)\n",
    "\n",
    "y_bar = model_transformer_decoder(x, y)\n",
    "print('Transformer decoder input size: {}' .format(x.shape))\n",
    "print('Transformer decoder output size: {}' .format(y_bar.shape))  \n",
    "print('-'*70)\n",
    "        \n",
    "        \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53396db1-58a7-45eb-a097-c3da68cf01a3",
   "metadata": {},
   "source": [
    "## TRAINING THE MODEL on the asked DATASET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7e9fd1a2-1669-4b35-b0b7-1adc420d4fad",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: datasets in /home/soham/.local/lib/python3.12/site-packages (3.3.1)\n",
      "Requirement already satisfied: filelock in /home/soham/.local/lib/python3.12/site-packages (from datasets) (3.17.0)\n",
      "Requirement already satisfied: numpy>=1.17 in /usr/lib/python3/dist-packages (from datasets) (1.26.4)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in /home/soham/.local/lib/python3.12/site-packages (from datasets) (19.0.0)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /home/soham/.local/lib/python3.12/site-packages (from datasets) (0.3.8)\n",
      "Requirement already satisfied: pandas in /usr/lib/python3/dist-packages (from datasets) (2.1.4+dfsg)\n",
      "Requirement already satisfied: requests>=2.32.2 in /home/soham/.local/lib/python3.12/site-packages (from datasets) (2.32.3)\n",
      "Requirement already satisfied: tqdm>=4.66.3 in /home/soham/.local/lib/python3.12/site-packages (from datasets) (4.67.1)\n",
      "Requirement already satisfied: xxhash in /home/soham/.local/lib/python3.12/site-packages (from datasets) (3.5.0)\n",
      "Requirement already satisfied: multiprocess<0.70.17 in /home/soham/.local/lib/python3.12/site-packages (from datasets) (0.70.16)\n",
      "Requirement already satisfied: fsspec<=2024.12.0,>=2023.1.0 in /home/soham/.local/lib/python3.12/site-packages (from fsspec[http]<=2024.12.0,>=2023.1.0->datasets) (2024.12.0)\n",
      "Requirement already satisfied: aiohttp in /home/soham/.local/lib/python3.12/site-packages (from datasets) (3.11.12)\n",
      "Requirement already satisfied: huggingface-hub>=0.24.0 in /home/soham/.local/lib/python3.12/site-packages (from datasets) (0.28.1)\n",
      "Requirement already satisfied: packaging in /usr/lib/python3/dist-packages (from datasets) (24.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/lib/python3/dist-packages (from datasets) (6.0.1)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /home/soham/.local/lib/python3.12/site-packages (from aiohttp->datasets) (2.4.6)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /home/soham/.local/lib/python3.12/site-packages (from aiohttp->datasets) (1.3.2)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /usr/lib/python3/dist-packages (from aiohttp->datasets) (23.2.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /home/soham/.local/lib/python3.12/site-packages (from aiohttp->datasets) (1.5.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /home/soham/.local/lib/python3.12/site-packages (from aiohttp->datasets) (6.1.0)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /home/soham/.local/lib/python3.12/site-packages (from aiohttp->datasets) (0.2.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in /home/soham/.local/lib/python3.12/site-packages (from aiohttp->datasets) (1.18.3)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/lib/python3/dist-packages (from huggingface-hub>=0.24.0->datasets) (4.10.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/soham/.local/lib/python3.12/site-packages (from requests>=2.32.2->datasets) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/lib/python3/dist-packages (from requests>=2.32.2->datasets) (3.6)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/lib/python3/dist-packages (from requests>=2.32.2->datasets) (2.0.7)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/lib/python3/dist-packages (from requests>=2.32.2->datasets) (2023.11.17)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install datasets --break-system-packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "091ad7bb-2708-4187-b4aa-65a95878f553",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: huggingface_hub in /home/soham/.local/lib/python3.12/site-packages (0.28.1)\n",
      "Requirement already satisfied: filelock in /home/soham/.local/lib/python3.12/site-packages (from huggingface_hub) (3.17.0)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /home/soham/.local/lib/python3.12/site-packages (from huggingface_hub) (2024.12.0)\n",
      "Requirement already satisfied: packaging>=20.9 in /usr/lib/python3/dist-packages (from huggingface_hub) (24.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/lib/python3/dist-packages (from huggingface_hub) (6.0.1)\n",
      "Requirement already satisfied: requests in /home/soham/.local/lib/python3.12/site-packages (from huggingface_hub) (2.32.3)\n",
      "Requirement already satisfied: tqdm>=4.42.1 in /home/soham/.local/lib/python3.12/site-packages (from huggingface_hub) (4.67.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/lib/python3/dist-packages (from huggingface_hub) (4.10.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/soham/.local/lib/python3.12/site-packages (from requests->huggingface_hub) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/lib/python3/dist-packages (from requests->huggingface_hub) (3.6)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/lib/python3/dist-packages (from requests->huggingface_hub) (2.0.7)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/lib/python3/dist-packages (from requests->huggingface_hub) (2023.11.17)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install huggingface_hub --break-system-packages"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a491d984-e71c-4ab1-8e52-fec22421d28f",
   "metadata": {},
   "source": [
    "### Transformer Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "69205ebf-0b30-459e-8313-a5216e850fdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerModel(nn.Module):\n",
    "    def __init__(self, src_vocab_size, tgt_vocab_size, dims_embd):\n",
    "        super().__init__()\n",
    "        self.embedding_src = nn.Embedding(src_vocab_size, dims_embd)\n",
    "        self.embedding_tgt = nn.Embedding(tgt_vocab_size, dims_embd)\n",
    "        \n",
    "        self.encoder = transformer_encoder(dims_embd)\n",
    "        self.decoder = transformer_decoder(dims_embd)\n",
    "        \n",
    "        self.output_layer = nn.Linear(dims_embd, tgt_vocab_size)\n",
    "\n",
    "    def forward(self, src, tgt):\n",
    "        # src_emb = self.embedding_src(src)\n",
    "        # tgt_emb = self.embedding_tgt(tgt)\n",
    "        src_emb = self.embedding_src(src).transpose(0, 1)  # (seq_len, batch) -> (batch, seq_len, embed)\n",
    "        tgt_emb = self.embedding_tgt(tgt).transpose(0, 1)\n",
    "\n",
    "        memory = self.encoder(src_emb)\n",
    "        out = self.decoder(memory, tgt_emb)\n",
    "        return self.output_layer(out)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "275bb893-44d3-4d82-a63e-387ae7027207",
   "metadata": {},
   "source": [
    "### Step 1: Loading Bengali and English sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "aca154c3-c2b7-4dff-bf10-1b7940db3410",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 72022 Bengali sentences and 72022 English sentences.\n"
     ]
    }
   ],
   "source": [
    "with open('bn-en.txt/OpenSubtitles.bn-en.bn', encoding='utf-8') as f_bn:\n",
    "    bn_sentences = f_bn.read().strip().split('\\n')\n",
    "\n",
    "with open('bn-en.txt/OpenSubtitles.bn-en.en', encoding='utf-8') as f_en:\n",
    "    en_sentences = f_en.read().strip().split('\\n')\n",
    "\n",
    "print(f\"Loaded {len(bn_sentences)} Bengali sentences and {len(en_sentences)} English sentences.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0601d4c9-12ef-4131-a62e-2bb7b7339c77",
   "metadata": {},
   "source": [
    "### Step 2: Build Vocabularies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "bce8355e-be07-4165-8e12-9517f3b1b6e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "def tokenize(text):\n",
    "    return text.lower().split()\n",
    "\n",
    "def build_vocab(sentences, min_freq=2):\n",
    "    counter = Counter()\n",
    "    for sentence in sentences:\n",
    "        tokens = tokenize(sentence)\n",
    "        counter.update(tokens)\n",
    "    \n",
    "    vocab = {\"<pad>\": 0, \"<sos>\": 1, \"<eos>\": 2, \"<unk>\": 3}\n",
    "    for word, freq in counter.items():\n",
    "        if freq >= min_freq:\n",
    "            vocab[word] = len(vocab)\n",
    "    \n",
    "    return vocab\n",
    "\n",
    "src_vocab = build_vocab(en_sentences)  # English is source\n",
    "tgt_vocab = build_vocab(bn_sentences)  # Bengali is target"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfda8045-7081-4da0-8d7e-7e0514139ac5",
   "metadata": {},
   "source": [
    "### Step 3: Encode Sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f3137267-3113-4222-84cd-c55e5ef8059b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode(sentence, vocab):\n",
    "    tokens = [\"<sos>\"] + tokenize(sentence) + [\"<eos>\"]\n",
    "    return torch.tensor([vocab.get(token, vocab[\"<unk>\"]) for token in tokens], dtype=torch.long)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c1108c1-b446-431f-8e87-9979882d9961",
   "metadata": {},
   "source": [
    "### Step 4: Create Dataset and DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d6fe3d07-9702-44d6-ab66-4eb1e11f34f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "class TranslationDataset(Dataset):\n",
    "    def __init__(self, src_sentences, tgt_sentences, src_vocab, tgt_vocab):\n",
    "        self.src_sentences = src_sentences\n",
    "        self.tgt_sentences = tgt_sentences\n",
    "        self.src_vocab = src_vocab\n",
    "        self.tgt_vocab = tgt_vocab\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.src_sentences)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        src = encode(self.src_sentences[idx], self.src_vocab)\n",
    "        tgt = encode(self.tgt_sentences[idx], self.tgt_vocab)\n",
    "        return src, tgt\n",
    "\n",
    "def collate_fn(batch):\n",
    "    src_batch, tgt_batch = zip(*batch)\n",
    "    src_batch = pad_sequence(src_batch, padding_value=src_vocab[\"<pad>\"], batch_first=False)\n",
    "    tgt_batch = pad_sequence(tgt_batch, padding_value=tgt_vocab[\"<pad>\"], batch_first=False)\n",
    "    return src_batch, tgt_batch\n",
    "\n",
    "dataset = TranslationDataset(en_sentences, bn_sentences, src_vocab, tgt_vocab)\n",
    "train_loader = DataLoader(dataset, batch_size=32, shuffle=True, collate_fn=collate_fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e029d911-429e-4f55-818e-ed24426a6ba7",
   "metadata": {},
   "source": [
    "### Step 5: Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6542af7d-0421-4f5b-8373-b74da5ca6505",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = TransformerModel(len(src_vocab), len(tgt_vocab), dims_embd=512).to(device)\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-4)\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=src_vocab[\"<pad>\"])\n",
    "\n",
    "num_epochs = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f209780-228d-4b46-921b-e24bf875445c",
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    epoch_loss = 0\n",
    "\n",
    "    for batch_idx, (src, tgt) in enumerate(train_loader):\n",
    "        src, tgt = src.to(device), tgt.to(device)\n",
    "\n",
    "        tgt_input = tgt[:-1, :]  # remove <eos>\n",
    "        tgt_output = tgt[1:, :]  # remove <sos>\n",
    "\n",
    "        preds = model(src, tgt_input)\n",
    "        preds = preds.view(-1, preds.shape[-1])\n",
    "        tgt_output = tgt_output.reshape(-1)\n",
    "\n",
    "        loss = criterion(preds, tgt_output)\n",
    "        loss.backward()\n",
    "\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        epoch_loss += loss.item()\n",
    "    \n",
    "    print(f\"Epoch {epoch+1}, Loss: {epoch_loss:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
